//! CLI for interacting with Gemini, 99% generated by Zed + Claude 3.7 Sonnet.

use anyhow::Result;
use clap::{Parser, Subcommand};
use futures::StreamExt;
use google_ai::{
    CacheContentsRequest, CacheName, Content, CountTokensRequest, GenerateContentRequest,
    GenerationConfig, Part, Role, SystemInstruction, TextPart, ToolConfig, UpdateCacheRequest,
    cache_contents, count_tokens, stream_generate_content, update_cache,
};
use reqwest_client::ReqwestClient;
use std::io::Write;
use std::{fs, io, path::PathBuf, sync::Arc, time::Duration};

#[derive(Parser)]
#[command(name = "google_ai_cli")]
#[command(author = "Zed Team")]
#[command(version = "0.1.0")]
#[command(about = "Interface with the Google Generative AI API", long_about = None)]
struct Cli {
    /// Google AI API key
    #[arg(long, env = "GOOGLE_AI_KEY")]
    api_key: String,

    /// API URL (defaults to https://generativelanguage.googleapis.com)
    #[arg(long, default_value = google_ai::API_URL)]
    api_url: String,

    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Generate content from a prompt
    Generate {
        /// The model to use (e.g., gemini-1.5-pro, gemini-1.5-flash)
        #[arg(long, default_value = "gemini-1.5-pro")]
        model: String,

        /// The prompt text
        #[arg(long)]
        prompt: String,

        /// System instruction for the model
        #[arg(long)]
        system_instruction: Option<String>,

        /// Maximum number of tokens to generate
        #[arg(long)]
        max_tokens: Option<usize>,

        /// Temperature for generation (0.0 to 1.0)
        #[arg(long)]
        temperature: Option<f64>,

        /// Top-p sampling parameter (0.0 to 1.0)
        #[arg(long)]
        top_p: Option<f64>,

        /// Top-k sampling parameter
        #[arg(long)]
        top_k: Option<usize>,
    },

    /// Count tokens in a prompt
    CountTokens {
        /// The prompt text
        #[arg(long)]
        prompt: String,
    },

    /// Cache content for faster repeated access
    CacheContents {
        /// The model to use
        #[arg(long)]
        model: String,

        /// The prompt text
        #[arg(long)]
        prompt: String,

        /// System instruction for the model
        #[arg(long)]
        system_instruction: String,

        /// Time-to-live for the cache in seconds
        #[arg(long, default_value = "3600")]
        ttl: u64,
    },

    /// Update cache TTL
    UpdateCache {
        /// The cache name to update
        #[arg(long)]
        name: String,

        /// New time-to-live for the cache in seconds
        #[arg(long)]
        ttl: u64,
    },

    /// Interactive conversation with the model
    Chat {
        /// The model to use (e.g., gemini-1.5-pro, gemini-1.5-flash)
        #[arg(long, default_value = "gemini-1.5-pro")]
        model: String,

        /// System instruction for the model
        #[arg(long)]
        system_instruction: Option<String>,

        /// Maximum number of tokens to generate
        #[arg(long)]
        max_tokens: Option<usize>,

        /// Temperature for generation (0.0 to 1.0)
        #[arg(long)]
        temperature: Option<f64>,

        /// Load a JSON file containing chat history
        #[arg(long)]
        history_file: Option<PathBuf>,
    },
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();
    let http_client = Arc::new(ReqwestClient::new());

    match &cli.command {
        Commands::Generate {
            model,
            prompt,
            system_instruction,
            max_tokens,
            temperature,
            top_p,
            top_k,
        } => {
            let user_content = Content {
                role: Role::User,
                parts: vec![Part::TextPart(TextPart {
                    text: prompt.clone(),
                })],
            };

            let mut request = GenerateContentRequest {
                model: model.clone(),
                contents: vec![user_content],
                system_instruction: system_instruction.as_ref().map(|instruction| {
                    SystemInstruction {
                        parts: vec![Part::TextPart(TextPart {
                            text: instruction.clone(),
                        })],
                    }
                }),
                generation_config: Some(GenerationConfig {
                    max_output_tokens: *max_tokens,
                    temperature: *temperature,
                    top_p: *top_p,
                    top_k: *top_k,
                    candidate_count: None,
                    stop_sequences: None,
                }),
                safety_settings: None,
                tools: None,
                tool_config: None,
            };

            println!("Generating content with model: {}", model);
            let mut stream =
                stream_generate_content(http_client.as_ref(), &cli.api_url, &cli.api_key, request)
                    .await?;

            println!("Response:");
            while let Some(response) = stream.next().await {
                match response {
                    Ok(resp) => {
                        if let Some(candidates) = &resp.candidates {
                            for candidate in candidates {
                                for part in &candidate.content.parts {
                                    match part {
                                        Part::TextPart(text_part) => {
                                            print!("{}", text_part.text);
                                        }
                                        _ => {
                                            println!("[Received non-text response part]");
                                        }
                                    }
                                }
                            }
                        }
                    }
                    Err(e) => {
                        eprintln!("Error: {}", e);
                        break;
                    }
                }
            }
            println!();
        }

        Commands::CountTokens { prompt } => {
            let request = CountTokensRequest {
                contents: vec![Content {
                    role: Role::User,
                    parts: vec![Part::TextPart(TextPart {
                        text: prompt.clone(),
                    })],
                }],
            };

            let response =
                count_tokens(http_client.as_ref(), &cli.api_url, &cli.api_key, request).await?;
            println!("Total tokens: {}", response.total_tokens);
        }

        Commands::CacheContents {
            model,
            prompt,
            system_instruction,
            ttl,
        } => {
            let request = CacheContentsRequest {
                ttl: Duration::from_secs(*ttl),
                model: model.clone(),
                contents: vec![Content {
                    role: Role::User,
                    parts: vec![Part::TextPart(TextPart {
                        text: prompt.clone(),
                    })],
                }],
                system_instruction: Content {
                    role: Role::User,
                    parts: vec![Part::TextPart(TextPart {
                        text: system_instruction.clone(),
                    })],
                },
                tools: Vec::new(),
                tool_config: ToolConfig {
                    function_calling_config: google_ai::FunctionCallingConfig {
                        mode: google_ai::FunctionCallingMode::None,
                        allowed_function_names: None,
                    },
                },
            };

            let response =
                cache_contents(http_client.as_ref(), &cli.api_url, &cli.api_key, request).await?;
            println!("Cache created:");
            println!("  Name: {:?}", response.name);
            println!("  Expires: {}", response.expire_time);
            if let Some(token_count) = response.usage_metadata.total_token_count {
                println!("  Total tokens: {}", token_count);
            }
        }

        Commands::UpdateCache { name, ttl } => {
            // We need to parse the cache name correctly with the prefix
            let cache_name_str = if name.starts_with("cachedContents/") {
                name.clone()
            } else {
                format!("cachedContents/{}", name)
            };

            // Create the cache name through the normal deserialization path
            let cache_name: CacheName =
                serde_json::from_value(serde_json::Value::String(cache_name_str))?;

            let request = UpdateCacheRequest {
                ttl: Duration::from_secs(*ttl),
            };

            let response = update_cache(
                http_client.as_ref(),
                &cli.api_url,
                &cli.api_key,
                &cache_name,
                request,
            )
            .await?;
            println!("Cache updated:");
            println!("  Name: {}", name);
            println!("  New expiration: {}", response.expire_time);
        }

        Commands::Chat {
            model,
            system_instruction,
            max_tokens,
            temperature,
            history_file,
        } => {
            // Initialize a vector to store conversation history
            let mut history = Vec::new();

            // Load history if provided
            if let Some(file_path) = history_file {
                if file_path.exists() {
                    let file_content = fs::read_to_string(file_path)?;
                    history = serde_json::from_str(&file_content)?;
                }
            }

            // Add system instruction if present
            if let Some(instruction) = system_instruction {
                println!("System: {}", instruction);
                println!();
            }

            loop {
                // Get user input
                print!("You: ");
                io::stdout().flush()?;
                let mut input = String::new();
                io::stdin().read_line(&mut input)?;

                if input.trim().eq_ignore_ascii_case("exit")
                    || input.trim().eq_ignore_ascii_case("quit")
                {
                    break;
                }

                // Add user message to history
                let user_content = Content {
                    role: Role::User,
                    parts: vec![Part::TextPart(TextPart {
                        text: input.trim().to_string(),
                    })],
                };
                history.push(user_content);

                // Create request with history
                let request = GenerateContentRequest {
                    model: model.clone(),
                    contents: history
                        .iter()
                        .map(|content| Content {
                            role: content.role,
                            parts: content
                                .parts
                                .iter()
                                .map(|part| match part {
                                    Part::TextPart(text_part) => Part::TextPart(TextPart {
                                        text: text_part.text.clone(),
                                    }),
                                    _ => panic!("Unsupported part type in history"),
                                })
                                .collect(),
                        })
                        .collect(),
                    system_instruction: system_instruction.as_ref().map(|instruction| {
                        SystemInstruction {
                            parts: vec![Part::TextPart(TextPart {
                                text: instruction.clone(),
                            })],
                        }
                    }),
                    generation_config: Some(GenerationConfig {
                        max_output_tokens: *max_tokens,
                        temperature: *temperature,
                        top_p: None,
                        top_k: None,
                        candidate_count: None,
                        stop_sequences: None,
                    }),
                    safety_settings: None,
                    tools: None,
                    tool_config: None,
                };

                // Get response
                print!("Assistant: ");
                let mut stream = stream_generate_content(
                    http_client.as_ref(),
                    &cli.api_url,
                    &cli.api_key,
                    request,
                )
                .await?;

                let mut model_response = String::new();
                while let Some(response) = stream.next().await {
                    match response {
                        Ok(resp) => {
                            if let Some(candidates) = &resp.candidates {
                                for candidate in candidates {
                                    for part in &candidate.content.parts {
                                        match part {
                                            Part::TextPart(text_part) => {
                                                print!("{}", text_part.text);
                                                model_response.push_str(&text_part.text);
                                            }
                                            _ => {
                                                println!("[Received non-text response part]");
                                            }
                                        }
                                    }
                                }
                            }
                        }
                        Err(e) => {
                            eprintln!("\nError: {}", e);
                            break;
                        }
                    }
                }
                println!();
                println!();

                // Add model response to history
                let model_content = Content {
                    role: Role::Model,
                    parts: vec![Part::TextPart(TextPart {
                        text: model_response,
                    })],
                };
                history.push(model_content);
            }
        }
    }

    Ok(())
}
