This eval revealed something fascinating: 85% of edits technically worked, but every single one had broken indentation. Models would strip leading spaces when copying, then generate at column zero. This eval failure drove us to build an algorithmic solution - calculate the indent difference and fix it on the fly. Once we had the algorithm, we could test it deterministically with our traditional approach. But we didn't stop there - we built property-based tests that generate random text with random indentation, split it into random chunks, and verify the algorithm handles any possible input correctly. The eval discovered the problem, we built a solution, tested that solution thoroughly, then went even further with property-based testing to ensure it's bulletproof. This pattern - eval reveals issue, build deterministic fix, test traditionally, then test exhaustively - shows how we layer our confidence.